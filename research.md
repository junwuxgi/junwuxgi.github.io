*Research areas of primary focus, collaborative engagement, or curiosity-driven*

## Scalable RL Reasoning

1. R1 /O1 related scalable RL reasoning algorithm and framework

## LLM Post-Training, such as RLHF, SFT

1. RLHF, RLAIF, RLXF
2. Reward Modeling
- Scale-law of Reward Modeling
- Reward Overoptimization / Reward Hacking(such as length hacking)

## LLM Pretraining
1. GPT Pretraining
2. MOE Pretraining(collaborative engagement)

## RL, Multi-Agent Learning Algorithm and Framework

1. Reward Modeling
- Reward shaping or tuning: Behavior Cloning/Inverse RL/Meta Learning/Imitation Learning
- Reward distribution: delay rewards, sparse rewards, noisy or biased rewards, misalignment, distribution shift
2. Off-policy and on-policy RL algorithms and framework
3. Multi-Task & Meta-Learning
4. Cooperated and competitive Multi-Agent learning algorithm and framework

## Reinforcement Preference Learning

1. Ranking
2. Pricing
3. Marketing
4. Recommendation algorithm and system

## Other Areas

Areas of curiosity-driven and collaborative engagement

1. AI alignment / Foundation model decision

- Multimodal alignment through RLXF

2. Agent Foundation Model/Scalable Agentic Alignment
3. Multimodal RL

- Multimodal Interaction

Areas of curiosity-driven

1. Controllable AIGC

- Diffusion Models

2. Aero/Embodied Agents/Robots
