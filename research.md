*Research areas of current primary focus, collaborative engagement, or curiosity-driven*

I am dedicated to cutting-edge research on **scalable reinforcement learning (RL) and agentic alignment from AI to AGI,** aiming to bridge the gap between specialized artificial intelligence and general-purpose autonomous systems through ethical, scalable, and adaptive frameworks.

I am currently advancing research on **scalable reinforcement learning methods** and **agentic alignment techniques** within **large language model (LLM) foundation models**, aiming to enhance their **complex reasoning capabilities**. His work focuses on developing advanced algorithms and frameworks that leverage high-quality data, including R1/O1-related scalable RL alignment algorithms, post-training methods such as **reinforcement learning with diverse feedbacks (RLXF)** and supervised fine-tuning (SFT), and broader AI alignment strategies. Additionally, he is actively involved in research on **multimodal interaction** and demonstrates a keen interest in **controllable AI-generated content (AIGC)**.

In my prior work, I have made valuable contributions to **reinforcement learning** and **multi-agent systems**, particularly through the development of **reward tuning**, **off-policy** and **on-policy RL algorithms and framework**, as well as algorithms for **cooperative** and **competitive multi-agent learning**. Furthermore, his research, which integrates **preference learning**, has been widely applied to practical domains such as **ranking**, **pricing**, **marketing**, and **recommendation systems**.

## Scalable RL Reasoning

1. R1 /O1 related scalable RL reasoning algorithm and framework

## LLM Post-Training, such as RLHF, SFT

1. RLHF, RLAIF, RLXF
2. Reward Modeling

- Scale-law of Reward Modeling
- Reward Overoptimization / Reward Hacking(such as length hacking)

## LLM Pretraining

1. GPT Pretraining
2. MOE Pretraining(collaborative engagement)

## RL, Multi-Agent Learning Algorithm and Framework

1. Reward Modeling

- Reward shaping or tuning: Behavior Cloning/Inverse RL/Meta Learning/Imitation Learning
- Reward distribution: delay rewards, sparse rewards, noisy or biased rewards, misalignment, distribution shift

2. Off-policy and on-policy RL algorithms and framework
3. Multi-Task & Meta-Learning
4. Cooperated and competitive Multi-Agent learning algorithm and framework

## Reinforcement Preference Learning

1. Ranking
2. Pricing
3. Marketing
4. Recommendation algorithm and system

## Other Areas

Areas of curiosity-driven and collaborative engagement

1. AI alignment / Foundation model decision

- Multimodal alignment through RLXF

2. Agent Foundation Model/Scalable Agentic Alignment
3. Multimodal RL

- Multimodal Interaction

Areas of curiosity-driven

1. Controllable AIGC

- Diffusion Models

2. Aero/Embodied Agents/Robots