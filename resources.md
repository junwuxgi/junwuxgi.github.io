### Open Scalable RL/Agentic Alignment Framework

**inclusionAI/AReaL**: Distributed RL System for LLM Reasoning, an open-source and efficient reinforcement learning system developed at the RL Lab, Ant Research, [git](https://github.com/inclusionAI/AReaL)

**veRL**: Volcano Engine Reinforcement Learning for LLM, a flexible, efficient and production-ready RL training library for large language models (LLMs), [git](https://github.com/volcengine/verl); Awesome work using verl:

* [Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization](https://arxiv.org/abs/2410.09302)
* [Flaming-hot Initiation with Regular Execution Sampling for Large Language Models](https://arxiv.org/abs/2410.21236)
* [Process Reinforcement Through Implicit Rewards](https://github.com/PRIME-RL/PRIME/)
* [TinyZero](https://github.com/Jiayi-Pan/TinyZero): a reproduction of DeepSeek R1 Zero recipe for reasoning tasks
* [RAGEN](https://github.com/ZihanWang314/ragen): a general-purpose reasoning agent training framework
* [Logic R1](https://github.com/Unakar/Logic-RL): a reproduced DeepSeek R1 Zero on 2K Tiny Logic Puzzle Dataset.
* [deepscaler](https://github.com/agentica-project/deepscaler): iterative context scaling with GRPO
* [critic-rl](https://github.com/HKUNLP/critic-rl): Teaching Language Models to Critique via Reinforcement Learning

**OpenRLHF**: a high-performance RLHF framework built on Ray, DeepSpeed and HF Transformers, [git](https://github.com/OpenRLHF/OpenRLHF)

**DeepSpeed**: empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales,[blog](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat), [git](https://github.com/deepspeedai/DeepSpeed)

**TRL** - Transformer Reinforcement Learning, [git](https://github.com/huggingface/trl)

### Open Scalable RL/Agentic Alignment Algorithms

**NVIDIA NeMo-Aligner**: Scalable toolkit for efficient model alignment, [git](https://github.com/NVIDIA/NeMo-Aligner)

**RLHFlow**: Open-Source Code for RLHF Workflow, [git](https://github.com/RLHFlow), [paper](https://arxiv.org/abs/2405.07863); Code for Reward Modeling: [git](https://github.com/RLHFlow/RLHF-Reward-Modeling); Code for Online RLHF: [git](https://github.com/RLHFlow/Online-RLHF); Online-DPO-R1: Unlocking Effective Reasoning Without the PPO Overhead, [git](https://github.com/RLHFlow/Online-DPO-R1)

### Open Scalable RL/Agentic Alignment Related Modules

**SGLang**: Efficient Execution of Structured Language Model Programs: [blog](https://rocm.blogs.amd.com/artificial-intelligence/sglang/README.html), [paper](https://arxiv.org/abs/2312.07104), [git.](https://github.com/sgl-project/sglang)

**vLLM**: A high-throughput and memory-efficient inference and serving engine for LLMs, [blog](https://blog.vllm.ai/2023/06/20/vllm.html), [git](https://github.com/vllm-project/vllm;)

Efficient Memory Management for Large Language Model Serving with PagedAttention, [paper](https://arxiv.org/pdf/2309.06180)

**Megatron-LM & Megatron-Core**: Ongoing research training transformer models at scale, [paper](https://arxiv.org/abs/1909.08053), [git](https://github.com/NVIDIA/Megatron-LM)

**Ray**: [Ray v2 Architecture doc](https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview?tab=t.0), [latest doc](https://docs.ray.io/en/latest/index.html), [git](https://github.com/ray-project/ray)
